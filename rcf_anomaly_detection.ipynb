{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing csv file and editing it to ahve another column  - experiment 1 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChristianL\\AppData\\Local\\Temp\\ipykernel_21056\\1389200449.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "input_file_name = 'DryerPerformance_03.csv'  # Name of your input CSV file\n",
    "output_file_name = 'DryerPerformance_03-output.csv'  # Name of the output CSV file with the additional column\n",
    "column_name = 'Dryer1LbsPerHr'  # The column based on whose values you're adding a 1 or 0\n",
    "new_column_name = 'RunningOptimally'  # Name of the new column to be added\n",
    "\n",
    "# Read the input CSV file and write to the output CSV file with the additional column\n",
    "with open(input_file_name, mode='r', newline='') as infile, open(output_file_name, mode='w', newline='') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames + [new_column_name]  # Add the new column name pip to the fieldnames\n",
    "    \n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for row in reader:\n",
    "        # Check the condition for the specific column and set the value of the new column accordingly\n",
    "        # Adjust the condition below according to your requirements\n",
    "        if float(row[column_name]) > 30000:\n",
    "            row[new_column_name] = '1'\n",
    "        else:\n",
    "            row[new_column_name] = '0'\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment 2 below (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'DryerPerformance_03-output.csv'  # Update this to the path of your CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# If necessary, convert your date column to datetime and sort by date\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])  # Replace 'Date' with your actual date column name\n",
    "data.sort_values('DateTime', inplace=True)\n",
    "\n",
    "# Replace 'Value' with the name of the column you want to analyze for anomalies\n",
    "column_name = 'Value'  # Update this to the name of your feature column\n",
    "\n",
    "# Isolation Forest for anomaly detection\n",
    "model = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
    "data['anomaly'] = model.fit_predict(data[[column_name]])\n",
    "\n",
    "# Visualize the data along with the anomalies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['Date'], data[column_name], color='blue', label='Normal')\n",
    "plt.scatter(data['Date'][data['anomaly'] == -1], data[column_name][data['anomaly'] == -1], color='red', label='Anomaly')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(column_name)\n",
    "plt.title('Time Series Anomaly Detection')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 3 below (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'DryerPerformance_03-output.csv'  # Update this with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming 'DateTime' is your datetime column and should be sorted\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.sort_values('DateTime', inplace=True)\n",
    "\n",
    "# Exclude 'RunningOptimally' from features to analyze\n",
    "features = data.columns.drop(['DateTime', 'RunningOptimally'])\n",
    "\n",
    "# Dictionary to hold anomaly counts for each feature\n",
    "anomaly_counts = {}\n",
    "\n",
    "for feature in features:\n",
    "    # Isolation Forest model\n",
    "    model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "    # Fit model on the feature\n",
    "    anomalies = model.fit_predict(data[[feature]])\n",
    "    # Convert anomalies to a binary flag (1 for anomaly, 0 for normal)\n",
    "    anomaly_flags = (anomalies == -1).astype(int)\n",
    "    \n",
    "    # Compare detected anomalies with 'RunningOptimally' to see alignment\n",
    "    comparison = anomaly_flags == data['RunningOptimally']\n",
    "    # Count how many times anomalies align with 'RunningOptimally'=0\n",
    "    anomaly_counts[feature] = comparison.value_counts().get(False, 0)\n",
    "\n",
    "# Sort features based on count of aligned anomalies\n",
    "sorted_anomaly_counts = sorted(anomaly_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print features ranked by their alignment with 'RunningOptimally'\n",
    "for feature, count in sorted_anomaly_counts:\n",
    "    print(f\"Feature: {feature}, Anomaly Alignments: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment 4 below (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load your dataset\n",
    "file_path = 'DryerPerformance_03-output.csv'  # Update this with your actual file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Assuming 'DateTime' is your datetime column and should be sorted\n",
    "data['DateTime'] = pd.to_datetime(data['DateTime'])\n",
    "data.sort_values('DateTime', inplace=True)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = data.drop(['DateTime', 'RunningOptimally'], axis=1)\n",
    "y = data['RunningOptimally']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the HistGradientBoostingClassifier\n",
    "model = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "# Compute permutation importance\n",
    "perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Get sorted feature importances\n",
    "sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
    "\n",
    "# Print features ranked by their permutation importance\n",
    "print(\"Feature ranking by permutation importance:\")\n",
    "for i in sorted_idx:\n",
    "    print(f\"{X.columns[i]}: {perm_importance.importances_mean[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment number 5 below (current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Feature  AnomaliesCount\n",
      "0                       OutfeedMoistureMW            2577\n",
      "1                  Zone3Sec9SteamSupPress            2572\n",
      "2                             WetBinSpeed            2503\n",
      "3                    Zone2SteamSupplyTemp            1555\n",
      "4            Zone3ExhaustAirDamperControl             888\n",
      "5                            SpeedLimitHz             811\n",
      "6                    Zone3SteamSupplyTemp             399\n",
      "7                    Zone2MakeupAirDamper             323\n",
      "8                    WetBinLaserLevelWest             161\n",
      "9                    Zone1RetChamberPress             152\n",
      "10                             Sec9DeltaT              86\n",
      "11                          HumidityZone2              38\n",
      "12                      Sec9SupplyAirTemp              35\n",
      "13                             Sec7DeltaT              22\n",
      "14           Zone2ExhaustAirDamperControl              14\n",
      "15                          HumidityZone1               3\n",
      "16                        OutfeedMoisture               2\n",
      "17                   Zone1ExhaustMoisture               1\n",
      "18                    Sec7SupplyAirTempSP               1\n",
      "19                 Sec7SupplyAirMpcTempSP               1\n",
      "20                          MatHeightEast               0\n",
      "21                          MatHeightWest               0\n",
      "22                         MoistureTarget               0\n",
      "23                             Sec1DeltaT               0\n",
      "24                         Sec1RetAirTemp               0\n",
      "25                         Sec1SupAirTemp               0\n",
      "26                             Sec2DeltaT               0\n",
      "27                         Sec2RetAirTemp               0\n",
      "28                         Sec2SupAirTemp               0\n",
      "29                             Sec3DeltaT               0\n",
      "30                         Sec3RetAirTemp               0\n",
      "31                         Sec3SupAirTemp               0\n",
      "32    Sec4BoosterCoilSteamSupplyContValve               0\n",
      "33   Sec4BoosterCoilSupplyAirTempSetpoint               0\n",
      "34                             Sec4DeltaT               0\n",
      "35       Sec4MainCoilSteamSupplyContValve               0\n",
      "36      Sec4MainCoilSupplyAirTempSetpoint               0\n",
      "37                         Sec4RetAirTemp               0\n",
      "38                Sec4SupplyAirTempTE2031               0\n",
      "39                Sec4SupplyAirTempTE2033               0\n",
      "40                             Sec5DeltaT               0\n",
      "41                         Sec5RetAirTemp               0\n",
      "42               Sec5SteamSupplyContValve               0\n",
      "43                  Sec5SupplyAirSetpoint               0\n",
      "44                      Sec5SupplyAirTemp               0\n",
      "45                             Sec6DeltaT               0\n",
      "46                         Sec6RetAirTemp               0\n",
      "47               Sec6SteamSupplyContValve               0\n",
      "48                  Sec6SupplyAirSetpoint               0\n",
      "49                      Sec6SupplyAirTemp               0\n",
      "50    Sec7BoosterCoilSteamSupplyContValve               0\n",
      "51   Sec7BoosterCoilSupplyAirTempSetpoint               0\n",
      "52       Sec7MainCoilSteamSupplyContValve               0\n",
      "53      Sec7MainCoilSupplyAirTempSetpoint               0\n",
      "54                         Sec7RetAirTemp               0\n",
      "55                      Sec7SupplyAirTemp               0\n",
      "56                             Sec8DeltaT               0\n",
      "57                         Sec8RetAirTemp               0\n",
      "58               Sec8SteamSupplyContValve               0\n",
      "59                  Sec8SupplyAirSetpoint               0\n",
      "60                      Sec8SupplyAirTemp               0\n",
      "61                         Sec9RetAirTemp               0\n",
      "62               Sec9SteamSupplyContValve               0\n",
      "63                  Sec9SupplyAirSetpoint               0\n",
      "64                                  Speed               0\n",
      "65                         ThruputCurrent               0\n",
      "66                      ThruputTotalShift               0\n",
      "67          Zone1AvgSupplySteamTempTE2225               0\n",
      "68                     Zone1BeltSpeed_1hz               0\n",
      "69                  Zone1ConveyorSpeedRef               0\n",
      "70                        Zone1EhaustTemp               0\n",
      "71           Zone1ExhaustAirDamperControl               0\n",
      "72           Zone1ExhaustMoistureSetpoint               0\n",
      "73                   Zone1MakeupAirDamper               0\n",
      "74                     Zone1MakeupAirTemp               0\n",
      "75                      Zone1MatHeightAvg               0\n",
      "76                     Zone1RetAirAvgTemp               0\n",
      "77           Zone1RetChamberPressSetpoint               0\n",
      "78                      Zone1SteamSupFlow               0\n",
      "79              Zone1SteamSupplyContValve               0\n",
      "80               Zone1SteamSupplySetpoint               0\n",
      "81                   Zone1SteamSupplyTemp               0\n",
      "82                     Zone1SteamSupPress               0\n",
      "83                     Zone1SupAirAvgTemp               0\n",
      "84                    Zone1SupAirSetpoint               0\n",
      "85                        Zone1SupAirTemp               0\n",
      "86                     Zone2BeltSpeed_1hz               0\n",
      "87           Zone2ConveyorSpeedAdjustment               0\n",
      "88                   Zone2ExhaustMoisture               0\n",
      "89           Zone2ExhaustMoistureSetpoint               0\n",
      "90                       Zone2ExhaustTemp               0\n",
      "91                     Zone2MakeupAirTemp               0\n",
      "92                         Zone2MatHeight               0\n",
      "93                     Zone2RetAirAvgTemp               0\n",
      "94           Zone2RetChamberPressSetpoint               0\n",
      "95                Zone2RetChamberPressure               0\n",
      "96                 Zone2Sec4SteamSupPress               0\n",
      "97                 Zone2Sec5SteamSupPress               0\n",
      "98                 Zone2Sec6SteamSupPress               0\n",
      "99                      Zone2SteamSupFlow               0\n",
      "100                    Zone2SteamSupPress               0\n",
      "101                    Zone2SupAirAvgTemp               0\n",
      "102                    Zone3BeltSpeed_1hz               0\n",
      "103          Zone3ConveyorSpeedAdjustment               0\n",
      "104                  Zone3ExhaustMoisture               0\n",
      "105          Zone3ExhaustMoistureSetpoint               0\n",
      "106                      Zone3ExhaustTemp               0\n",
      "107                    Zone3ExitBeltSpeed               0\n",
      "108                  Zone3MakeupAirDamper               0\n",
      "109                    Zone3MakeupAirTemp               0\n",
      "110                        Zone3MatHeight               0\n",
      "111                    Zone3RetAirAvgTemp               0\n",
      "112        Zone_3_Return_Chamber_Pressure               0\n",
      "113          Zone3RetChamberPressSetpoint               0\n",
      "114                Zone3Sec7SteamSupPress               0\n",
      "115                Zone3Sec8SteamSupPress               0\n",
      "116                     Zone3SteamSupFlow               0\n",
      "117                    Zone3SteamSupPress               0\n",
      "118                    Zone3SupAirAvgTemp               0\n",
      "119                 MoistureControlManual               0\n",
      "120                       Zone2PressurePV               0\n",
      "121                       Zone2PressureSP               0\n",
      "122                      Zone2PressureOUT               0\n",
      "123                           WetBinLevel               0\n",
      "124                      FlakerFeedingWB1               0\n",
      "125                     Zone1HzActWithMat               0\n",
      "126                     MakeUpAirFanOnOff               0\n",
      "127                         Dryer1Running               0\n",
      "128                        InfeedMoisture               0\n",
      "129                        Sec1WestFanLow               0\n",
      "130                       Sec1WestFanHigh               0\n",
      "131       Dryer1Zone1SteamSupContValveOUT               0\n",
      "132                           MpcOnDryer1               0\n",
      "133                         MpcMcSpDryer1               0\n",
      "134                  WetBinLaserLevelEast               0\n",
      "135                    Dryer1MaxBeltSpeed               0\n",
      "136                        MpcConstrValve               0\n",
      "137                         MpcConstrTemp               0\n",
      "138                        MpcConstrSteam               0\n",
      "139                       MpcConstrWetBin               0\n",
      "140                         HumidityZone3               0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 4000\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('DryerPerformance_03-output.csv')\n",
    "\n",
    "# Filter rows where 'RunningOptimally' is 0\n",
    "df_not_optimal = df[df['RunningOptimally'] == 0]\n",
    "\n",
    "# Initialize a dictionary to hold the count of anomalies for each feature\n",
    "anomalies_count = {}\n",
    "\n",
    "# Define an anomaly threshold (e.g., 2 standard deviations from the mean)\n",
    "threshold = 6\n",
    "\n",
    "# Loop through each feature column (excluding 'DateTime' and 'RunningOptimally')\n",
    "for column in df_not_optimal.columns.drop(['DateTime', 'RunningOptimally', 'Dryer1LbsPerHr']):\n",
    "    # Calculate the mean and standard deviation for the column\n",
    "    mean = df_not_optimal[column].mean()\n",
    "    std = df_not_optimal[column].std()\n",
    "    \n",
    "    # Define what is considered an anomaly (outside of threshold standard deviations)\n",
    "    lower_bound = mean - threshold * std\n",
    "    upper_bound = mean + threshold * std\n",
    "    \n",
    "    # Count how many values fall outside of the bounds (anomalies)\n",
    "    anomalies = df_not_optimal[(df_not_optimal[column] < lower_bound) | (df_not_optimal[column] > upper_bound)]\n",
    "    anomalies_count[column] = len(anomalies)\n",
    "\n",
    "# Sort the dictionary by the count of anomalies in descending order\n",
    "sorted_anomalies = sorted(anomalies_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Convert to DataFrame for better visualization (optional)\n",
    "sorted_anomalies_df = pd.DataFrame(sorted_anomalies, columns=['Feature', 'AnomaliesCount'])\n",
    "\n",
    "# Display the sorted list of features by anomalies count\n",
    "print(sorted_anomalies_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================================================================\n",
    "\n",
    "Data to use for RCF test model below\n",
    "\n",
    "https://joanna-canvas.s3.amazonaws.com/canvas_dryer1_performance/DryerPerformance_03-output.csv\n",
    "arn:aws:s3:::joanna-canvas/canvas_dryer1_performance/DryerPerformance_03-output.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection for SaaS metrics using Random Cut Forests (RCF)\n",
    "***\n",
    "\n",
    "## Introduction\n",
    "Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. They are easy to describe in that, when viewed in a plot, they are often easily distinguishable from the \"regular\" data. Including these anomalies in a data set can drastically increase the complexity of a machine learning task since the \"regular\" data can often be described with a simple model.\n",
    "\n",
    "Find the documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html).\n",
    "\n",
    "Deep dive video about RCF https://www.youtube.com/watch?v=9BWHR4JsTNU\n",
    "\n",
    "This notebook is based on the [\"Introduction to RCF\" notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/random_cut_forest/random_cut_forest.ipynb). If you want more details on what is done here, feel free to check it out.\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "*This notebook was tested in Amazon SageMaker Studio instance with Python 3 (conda_python3 kernel).*\n",
    "\n",
    "\n",
    "First, ensure Sagemaker has read access to S3 and Athena.\n",
    "Then adjust the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (619133805.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    s3_bucket = # \"USERNAME-tenant-metrics\" # an Amazon S3 bucket accessible by your account\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO: adjust s3_bucket and athena_db\n",
    "s3_bucket = # \"USERNAME-tenant-metrics\" # an Amazon S3 bucket accessible by your account \n",
    "s3_prefix = \"sagemaker/rcf\"\n",
    "athena_db = \"\\\"tenants-metrics\\\".\\\"sample_data\\\"\" #Adjust athena_db if needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding import of system required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# aws\n",
    "import boto3\n",
    "import botocore\n",
    "#!conda install --yes --prefix {sys.prefix} PyAthena\n",
    "!{sys.executable} -m pip install PyAthena\n",
    "from pyathena import connect\n",
    "import sagemaker\n",
    "from sagemaker import RandomCutForest, Session, get_execution_role\n",
    "\n",
    "# graphs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize session\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "execution_role = get_execution_role()\n",
    "\n",
    "# define paths\n",
    "s3_query_results = f's3://{s3_bucket}/{s3_prefix}/query-results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=s3_bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('Hey! You either forgot to specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"Hey! You don't have permission to access the bucket, {}.\".format(s3_bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"Hey! Your bucket, {}, doesn't exist!\".format(s3_bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(s3_bucket, s3_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying S3 using Athena\n",
    "The query gets the storage consumption per day of a certain tenant sorted by time. \n",
    "You can read more about querying S3 using Athena from Sagemaker over [here](https://aws.amazon.com/blogs/machine-learning/run-sql-queries-from-your-sagemaker-notebooks-using-amazon-athena/).\n",
    "\n",
    "When plotting the data, it becomes clear that there are indeed some anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = connect(s3_staging_dir=s3_query_results, region_name=region)\n",
    "\n",
    "# queries the storage consumption per day of tenant 3\n",
    "query = f\"\"\"\n",
    "WITH storage_consumption AS(\n",
    "    SELECT\n",
    "        date_trunc('day', from_unixtime(timestamp)) AS timestamp,\n",
    "        metric.value                                AS storage\n",
    "    FROM\n",
    "        { athena_db }\n",
    "    WHERE\n",
    "        metric.name = 'Storage'\n",
    "        AND\n",
    "            tenant.id = 'tenant-id-3'\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    storage_consumption.timestamp,\n",
    "    sum(storage)                   AS storage\n",
    "FROM \n",
    "    storage_consumption\n",
    "GROUP BY\n",
    "    storage_consumption.timestamp\n",
    "ORDER BY\n",
    "    storage_consumption.timestamp ASC\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, conn, index_col=\"timestamp\", parse_dates=\"timestamp\")\n",
    "df = df[1:-1] # skip first and last day, because only half the day was monitored\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Sagemaker and RCFs\n",
    "Setup sagemaker to store the training releated data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      instance_count=3,\n",
    "                      instance_type='ml.m4.xlarge',\n",
    "                      use_spot_instances=True,  # Use a spot instance \n",
    "                      max_run=300,  # Max training time\n",
    "                      max_wait=600,  # Max training time + spot waiting time\n",
    "                      data_location='s3://{}/{}/{}/'.format(s3_bucket, s3_prefix, \"rcf\"),\n",
    "                      output_path='s3://{}/{}/{}/output'.format(s3_bucket, s3_prefix, \"rcf\"),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RCF Model Training\n",
    "Train the RCF on our data and publish an endpoint. This will take around 15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# convert the data to a numpy array\n",
    "y = df.to_numpy()\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(y))\n",
    "print('Training job name: {}'.format(rcf.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Progress:\")\n",
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")\n",
    "print('Endpoint name: {}'.format(rcf_inference.endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "Run the prediction through the endpoint with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = rcf_inference.predict(y)\n",
    "scores = [datum.label[\"score\"].float32_tensor.values[0] for datum in results]\n",
    "\n",
    "results = df.copy()\n",
    "results[\"score\"] = scores\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(results['storage'], color='C0', alpha=0.8)\n",
    "ax2.plot(results['score'], color='C1')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "\n",
    "ax1.set_ylabel('Storage Consumption', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "\n",
    "ax2.set_ylim(min(scores), 1.4 * max(scores))\n",
    "fig.set_figwidth(16)\n",
    "\n",
    "score_mean = results['score'].mean()\n",
    "score_std = results['score'].std()\n",
    "score_cutoff = score_mean + 4 * score_std # adjust constant to tweak sensitivity\n",
    "\n",
    "anomalies = results[results['score'] > score_cutoff]\n",
    "ax2.plot(anomalies.index, anomalies.score, 'ko');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to delete the inference endpoint\n",
    "Session().delete_endpoint(rcf_inference.endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
